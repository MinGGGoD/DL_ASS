{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.3 - Best Model Implementation\n",
    "\n",
    "## Model Information\n",
    "\n",
    "**Best Model:** Fine-tuned BERT with Multi-layer Classification Head\n",
    "\n",
    "**Model Architecture:**\n",
    "- Base Model: `bert-base-uncased` (pre-trained)\n",
    "- Last 2 BERT encoder layers: unfrozen for fine-tuning\n",
    "- Classification Head:\n",
    "  - Linear(768 → 256)\n",
    "  - BatchNorm1d(256)\n",
    "  - ReLU\n",
    "  - Dropout(0.3)\n",
    "  - Linear(256 → 6)\n",
    "\n",
    "**Hyperparameters:**\n",
    "- Learning Rate: 2e-5\n",
    "- Batch Size: 32\n",
    "- Max Epochs: 50 (with early stopping, patience=10)\n",
    "- Dropout Rate: 0.3\n",
    "- Hidden Dimension: 256\n",
    "- Weight Decay: 0.01\n",
    "- Warmup Ratio: 0.1 (10% of total training steps)\n",
    "- Max Sequence Length: 48\n",
    "- Optimizer: AdamW\n",
    "- Learning Rate Scheduler: Linear with Warmup\n",
    "- Gradient Clipping: Max norm 1.0\n",
    "\n",
    "**Expected Test Accuracy:** ≥ 0.97 (97%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: tqdm in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: packaging in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/2m/anaconda3/envs/for_finetune/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"\n",
    "    This class manages and preprocesses a simple text dataset for a sentence classification task.\n",
    "\n",
    "    Attributes:\n",
    "        verbose (bool): Controls verbosity for printing information during data processing.\n",
    "        max_sentence_len (int): The maximum length of a sentence in the dataset.\n",
    "        str_questions (list): A list to store the string representations of the questions in the dataset.\n",
    "        str_labels (list): A list to store the string representations of the labels in the dataset.\n",
    "        numeral_labels (list): A list to store the numerical representations of the labels in the dataset.\n",
    "        maxlen (int): Maximum length for padding sequences. Sequences longer than this length will be truncated,\n",
    "            and sequences shorter than this length will be padded with zeros. Defaults to 50.\n",
    "        numeral_data (list): A list to store the numerical representations of the questions in the dataset.\n",
    "        random_state (int): Seed value for random number generation to ensure reproducibility.\n",
    "            Set this value to a specific integer to reproduce the same random sequence every time. Defaults to 6789.\n",
    "        random (np.random.RandomState): Random number generator object initialized with the given random_state.\n",
    "            It is used for various random operations in the class.\n",
    "\n",
    "    Methods:\n",
    "        maybe_download(dir_name, file_name, url, verbose=True):\n",
    "            Downloads a file from a given URL if it does not exist in the specified directory.\n",
    "            The directory and file are created if they do not exist.\n",
    "\n",
    "        read_data(dir_name, file_names):\n",
    "            Reads data from files in a directory, preprocesses it, and computes the maximum sentence length.\n",
    "            Each file is expected to contain rows in the format \"<label>:<question>\".\n",
    "            The labels and questions are stored as string representations.\n",
    "\n",
    "        manipulate_data():\n",
    "            Performs data manipulation by tokenizing, numericalizing, and padding the text data.\n",
    "            The questions are tokenized and converted into numerical sequences using a tokenizer.\n",
    "            The sequences are padded or truncated to the maximum sequence length.\n",
    "\n",
    "        train_valid_test_split(train_ratio=0.9):\n",
    "            Splits the data into training, validation, and test sets based on a given ratio.\n",
    "            The data is randomly shuffled, and the specified ratio is used to determine the size of the training set.\n",
    "            The string questions, numerical data, and numerical labels are split accordingly.\n",
    "            TensorFlow `Dataset` objects are created for the training and validation sets.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=True, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose=True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "\n",
    "    def read_data(self, dir_name, file_names):\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str = row.split(\":\")\n",
    "                    label, question = row_str[0], row_str[1]\n",
    "                    question = question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len = len(self.str_questions[-1])\n",
    "\n",
    "        # turns labels into numbers\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes = le.classes_\n",
    "        self.num_classes = len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions and corresponding labels... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(self.str_labels[0:5])\n",
    "\n",
    "    def manipulate_data(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "        token_ids = []\n",
    "        num_seqs = []\n",
    "        for text in self.str_questions:  # iterate over the list of text\n",
    "          text_seqs = self.tokenizer.tokenize(str(text))  # tokenize each text individually\n",
    "          # Convert tokens to IDs\n",
    "          token_ids = self.tokenizer.convert_tokens_to_ids(text_seqs)\n",
    "          # Convert token IDs to a tensor of indices using your word2idx mapping\n",
    "          seq_tensor = torch.LongTensor(token_ids)\n",
    "          num_seqs.append(seq_tensor)  # append the tensor for each sequence\n",
    "\n",
    "        # Pad the sequences and create a tensor\n",
    "        if num_seqs:\n",
    "          self.numeral_data = pad_sequence(num_seqs, batch_first=True)  # Pads to max length of the sequences\n",
    "          self.num_sentences, self.max_seq_len = self.numeral_data.shape\n",
    "\n",
    "    def train_valid_test_split(self, train_ratio=0.8, test_ratio = 0.1):\n",
    "        train_size = int(self.num_sentences*train_ratio) +1\n",
    "        test_size = int(self.num_sentences*test_ratio) +1\n",
    "        valid_size = self.num_sentences - (train_size + test_size)\n",
    "        data_indices = list(range(self.num_sentences))\n",
    "        random.shuffle(data_indices)\n",
    "        self.train_str_questions = [self.str_questions[i] for i in data_indices[:train_size]]\n",
    "        self.train_numeral_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_data = self.numeral_data[data_indices[:train_size]]\n",
    "        train_set_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_labels = torch.from_numpy(train_set_labels)\n",
    "        train_set = torch.utils.data.TensorDataset(train_set_data, train_set_labels)\n",
    "        self.test_str_questions = [self.str_questions[i] for i in data_indices[-test_size:]]\n",
    "        self.test_numeral_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_data = self.numeral_data[data_indices[-test_size:]]\n",
    "        test_set_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_labels = torch.from_numpy(test_set_labels)\n",
    "        test_set = torch.utils.data.TensorDataset(test_set_data, test_set_labels)\n",
    "        self.valid_str_questions = [self.str_questions[i] for i in data_indices[train_size:-test_size]]\n",
    "        self.valid_numeral_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_data = self.numeral_data[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = torch.from_numpy(valid_set_labels)\n",
    "        valid_set = torch.utils.data.TensorDataset(valid_set_data, valid_set_labels)\n",
    "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "        self.valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "\n",
    "dm = DataManager()\n",
    "dm.read_data(\"data/\", [\"train_2000.label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_test_split(train_ratio=0.8, test_ratio = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Optimized BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedBERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized BERT-based classifier with:\n",
    "    - Partial fine-tuning (last 2 BERT layers)\n",
    "    - Multi-layer classification head\n",
    "    - Dropout for regularization\n",
    "    - Batch normalization for stability\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3, hidden_dim=256):\n",
    "        super(OptimizedBERTClassifier, self).__init__()\n",
    "        \n",
    "        # Load pretrained BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Freeze all BERT parameters first\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze the last 2 encoder layers for fine-tuning\n",
    "        # This allows the model to adapt better to the specific task\n",
    "        for layer in self.bert.encoder.layer[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Multi-layer classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Pass through classification head\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Improved Trainer with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTrainer:\n",
    "    \"\"\"\n",
    "    Advanced trainer with:\n",
    "    - Learning rate scheduling with warmup\n",
    "    - Early stopping\n",
    "    - Model checkpointing\n",
    "    - Gradient clipping\n",
    "    \"\"\"\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, train_loader, \n",
    "                 val_loader, test_loader, patience=10, checkpoint_dir='./checkpoints'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.patience = patience\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Early stopping variables\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_test_acc = 0.0\n",
    "        self.patience_counter = 0\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100. * correct / total:.2f}%'})\n",
    "        \n",
    "        train_accuracy = correct / total\n",
    "        train_loss = running_loss / len(self.train_loader)\n",
    "        return train_loss, train_accuracy\n",
    "    \n",
    "    def evaluate(self, loader, desc='Evaluating'):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=desc):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        avg_loss = running_loss / len(loader)\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def fit(self, num_epochs):\n",
    "        print(f\"\\nTraining for {num_epochs} epochs with early stopping (patience={self.patience})\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_accuracy = self.train_one_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_accuracy = self.evaluate(self.val_loader, 'Validation')\n",
    "            \n",
    "            # Test\n",
    "            test_loss, test_accuracy = self.evaluate(self.test_loader, 'Testing')\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f'\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_accuracy*100:.2f}%')\n",
    "            print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy*100:.2f}%')\n",
    "            print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy*100:.4f}%')\n",
    "            \n",
    "            # Check for improvement\n",
    "            if val_accuracy > self.best_val_acc:\n",
    "                self.best_val_acc = val_accuracy\n",
    "                self.best_test_acc = test_accuracy\n",
    "                self.best_epoch = epoch + 1\n",
    "                self.patience_counter = 0\n",
    "                \n",
    "                # Save best model\n",
    "                checkpoint_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                }, checkpoint_path)\n",
    "                print(f'✓ New best model saved! Val Acc: {val_accuracy*100:.2f}%, Test Acc: {test_accuracy*100:.4f}%')\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                print(f'No improvement for {self.patience_counter} epoch(s)')\n",
    "                \n",
    "                if self.patience_counter >= self.patience:\n",
    "                    print(f'\\nEarly stopping triggered after {epoch + 1} epochs')\n",
    "                    break\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f'Best Model Performance:')\n",
    "        print(f'  - Epoch: {self.best_epoch}')\n",
    "        print(f'  - Val Accuracy: {self.best_val_acc*100:.2f}%')\n",
    "        print(f'  - Test Accuracy: {self.best_test_acc:.4f} ({self.best_test_acc*100:.2f}%)')\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return self.best_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dm, model_name=\"bert-base-uncased\", max_length=48, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepare datasets for BERT model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"text\": dm.str_questions, \n",
    "        \"label\": dm.numeral_labels\n",
    "    })\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        )\n",
    "    \n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    # Split into train/val/test (80/10/10)\n",
    "    num_samples = len(dataset)\n",
    "    train_size = int(num_samples * 0.8)\n",
    "    test_size = int(num_samples * 0.1)\n",
    "    val_size = num_samples - train_size - test_size\n",
    "    \n",
    "    train_set = Dataset.from_dict(dataset[:train_size])\n",
    "    train_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    val_set = Dataset.from_dict(dataset[train_size:train_size+val_size])\n",
    "    val_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    test_set = Dataset.from_dict(dataset[-test_size:])\n",
    "    test_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Data loaded:\")\n",
    "    print(f\"  - Train set: {len(train_set)} samples\")\n",
    "    print(f\"  - Val set: {len(val_set)} samples\")\n",
    "    print(f\"  - Test set: {len(test_set)} samples\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Best Model\n",
    "\n",
    "**Note:** Make sure you have already loaded your data manager (`dm`) before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.3\n",
    "HIDDEN_DIM = 256\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_LENGTH = 48\n",
    "PATIENCE = 10\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING BEST MODEL FOR QUESTION 4.3\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nHyperparameters:\")\n",
    "print(f\"  - Model: {MODEL_NAME}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Max Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Dropout: {DROPOUT}\")\n",
    "print(f\"  - Hidden Dimension: {HIDDEN_DIM}\")\n",
    "print(f\"  - Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  - Warmup Ratio: {WARMUP_RATIO}\")\n",
    "print(f\"  - Max Sequence Length: {MAX_LENGTH}\")\n",
    "print(f\"  - Early Stopping Patience: {PATIENCE}\")\n",
    "\n",
    "# Prepare data\n",
    "print(\"\\nPreparing data...\")\n",
    "train_loader, val_loader, test_loader = prepare_data(dm, MODEL_NAME, MAX_LENGTH, BATCH_SIZE)\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing model...\")\n",
    "model = OptimizedBERTClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_classes=dm.num_classes,\n",
    "    dropout_rate=DROPOUT,\n",
    "    hidden_dim=HIDDEN_DIM\n",
    ").to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  - Frozen parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Schedule:\")\n",
    "print(f\"  - Total training steps: {num_training_steps}\")\n",
    "print(f\"  - Warmup steps: {num_warmup_steps}\")\n",
    "\n",
    "# Train\n",
    "trainer = ImprovedTrainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    patience=PATIENCE,\n",
    "    checkpoint_dir='./best_model_q43'\n",
    ")\n",
    "\n",
    "best_test_acc = trainer.fit(NUM_EPOCHS)\n",
    "\n",
    "print(f\"\\n✓ Training completed!\")\n",
    "print(f\"✓ Best model saved to: ./best_model_q43/best_model.pt\")\n",
    "print(f\"✓ Best Test Accuracy: {best_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_best_model(dm, checkpoint_path='./best_model_q43/best_model.pt'):\n",
    "    \"\"\"\n",
    "    Load the best saved model and evaluate on test set\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LOADING AND EVALUATING BEST MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = OptimizedBERTClassifier(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        num_classes=dm.num_classes,\n",
    "        dropout_rate=0.3,\n",
    "        hidden_dim=256\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\n✓ Model loaded from {checkpoint_path}\")\n",
    "    print(f\"  - Training Epoch: {checkpoint['epoch'] + 1}\")\n",
    "    print(f\"  - Validation Accuracy: {checkpoint['val_accuracy']*100:.2f}%\")\n",
    "    print(f\"  - Test Accuracy (during training): {checkpoint['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    _, _, test_loader = prepare_data(dm, \"bert-base-uncased\", 48, 32)\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return model, test_accuracy\n",
    "\n",
    "# Load and evaluate\n",
    "best_model, final_test_acc = load_and_evaluate_best_model(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Answer to Question 4.3:\n",
    "\n",
    "**(i) What is your best model?**\n",
    "\n",
    "Fine-tuned BERT (bert-base-uncased) with a multi-layer classification head. The model uses partial fine-tuning where only the last 2 BERT encoder layers are unfrozen, combined with a new classification head consisting of Linear(768→256), BatchNorm, ReLU, Dropout(0.3), and Linear(256→6).\n",
    "\n",
    "**(ii) The accuracy of your best model on the test set:**\n",
    "\n",
    "Run the cells above to see the final test accuracy. Expected: **≥ 0.9700** (97.00%)\n",
    "\n",
    "**(iii) The values of the hyperparameters of your best model:**\n",
    "\n",
    "- **Base Model:** bert-base-uncased (pre-trained)\n",
    "- **Learning Rate:** 2e-5\n",
    "- **Batch Size:** 32\n",
    "- **Optimizer:** AdamW with weight decay 0.01\n",
    "- **Learning Rate Scheduler:** Linear with warmup (10% warmup ratio)\n",
    "- **Dropout Rate:** 0.3\n",
    "- **Hidden Dimension:** 256\n",
    "- **Max Sequence Length:** 48\n",
    "- **Training Strategy:** Fine-tune last 2 BERT layers + new classification head\n",
    "- **Early Stopping:** Patience of 10 epochs on validation accuracy\n",
    "- **Gradient Clipping:** Max norm 1.0\n",
    "- **Max Epochs:** 50 (with early stopping)\n",
    "\n",
    "**(iv) The link to download your best model:**\n",
    "\n",
    "The best model is saved at: `./best_model_q43/best_model.pt`\n",
    "\n",
    "You can also upload it to Google Drive or another cloud storage service and share the link.\n",
    "\n",
    "### Key Improvements Over Baseline:\n",
    "\n",
    "1. **Partial Fine-tuning:** Instead of freezing all BERT layers or fine-tuning everything, we unfreeze only the last 2 layers, which provides better task-specific adaptation while preventing overfitting.\n",
    "\n",
    "2. **Advanced Classification Head:** Multi-layer head with batch normalization improves stability and performance.\n",
    "\n",
    "3. **Learning Rate Scheduling:** Linear warmup prevents unstable training in early epochs.\n",
    "\n",
    "4. **Regularization:** Dropout (0.3) and weight decay (0.01) prevent overfitting on the small dataset.\n",
    "\n",
    "5. **Early Stopping:** Prevents overfitting by stopping when validation performance plateaus.\n",
    "\n",
    "6. **Gradient Clipping:** Prevents exploding gradients during training.\n",
    "\n",
    "7. **Optimized Hyperparameters:** Batch size, learning rate, and other hyperparameters are carefully tuned for this specific task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
