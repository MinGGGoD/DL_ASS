{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqMi8gdDBD1L"
   },
   "source": [
    "# <font color=\"#0b486b\">  Student Information</font>\n",
    "***\n",
    "Surname: **Zhang**  <br/>\n",
    "Firstname: **Yiming**    <br/>\n",
    "Student ID: **35224436**    <br/>\n",
    "Email: **yzha1213@student.monash.edu**    <br/>\n",
    "Your tutorial time: **12pm Wed**    <br/>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsB3OmMxB4Dh"
   },
   "source": [
    "## Section 1: Fundamentals in RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c83ea5R9nh_y"
   },
   "source": [
    "You need to **manually** implement a multi-timestep Recurrent Neural Network that can take an input as a 3D tensor `[batch_size, seq_len, input_size]` for a classification task.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[Total: 10 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OzH6RTIjExy2"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKlCx1D3rczS"
   },
   "source": [
    "We declare the relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "C77pPiGBA9IJ"
   },
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "seq_len = 4\n",
    "batch_size = 8\n",
    "hidden_size = 3\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A10tCeyKrkUt"
   },
   "source": [
    "We create random inputs (i.e., `inputs`) and random labels (i.e., `random_labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybfqj-wBEu0H",
    "outputId": "6af22c20-79f1-4374-9365-8e1c4e740bb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 5])\n",
      "tensor([2, 0, 1, 0, 2, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(batch_size, seq_len, input_size)\n",
    "random_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "print(inputs.shape)\n",
    "print(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7ABTPhyr6wa"
   },
   "source": [
    "(1) In what follows, we need to declare the model parameters, which include the matrices $U$ (``[input_size, hidden_size]``), W (``[hidden_size, hidden_size]``), $V$ (``[hidden_size, num_classes]``) and the biases $b$ and $c$ for the hidden states and logits respectively.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qzlZtACzsbSR"
   },
   "outputs": [],
   "source": [
    "#Insert your code here\n",
    "U = torch.randn(input_size, hidden_size, requires_grad=True)\n",
    "W = torch.randn(hidden_size, hidden_size, requires_grad=True)\n",
    "b = torch.randn(hidden_size, requires_grad=True)\n",
    "V = torch.randn(hidden_size, num_classes, requires_grad=True)\n",
    "c = torch.randn(num_classes, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MW_gal7Us9fK"
   },
   "source": [
    "(2) Next you need to write the code to compute `hiddens` which is a 3D tensor of the shape ``[batch_size, seq_len, hidden_size]`` using the formula of the simple/standard RNN cells. You can freely modify the code below.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwxqdXkTtaea",
    "outputId": "db3ed92a-d575-4771-e718-9c11fbd35ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0428,  0.7521, -0.9812],\n",
      "         [-0.9639,  0.9936, -0.9976],\n",
      "         [ 1.0000, -0.6238, -0.1974],\n",
      "         [-0.9973,  0.9863, -0.9990]],\n",
      "\n",
      "        [[-0.3650,  0.9942, -0.9898],\n",
      "         [ 1.0000,  0.9872,  0.4011],\n",
      "         [-0.9963,  0.9991, -0.9998],\n",
      "         [ 1.0000, -0.7119, -0.6415]],\n",
      "\n",
      "        [[ 0.9997, -0.9653, -0.7377],\n",
      "         [ 0.9995,  0.9999, -0.9084],\n",
      "         [ 0.8530,  0.9987, -0.9945],\n",
      "         [ 0.9999,  1.0000, -0.9446]],\n",
      "\n",
      "        [[ 0.8079, -0.6269, -0.9697],\n",
      "         [-0.9699,  0.9580, -0.9966],\n",
      "         [ 0.9927, -0.1821, -0.9440],\n",
      "         [ 0.9657,  0.8308, -0.9342]],\n",
      "\n",
      "        [[-0.0351,  0.1101, -0.9820],\n",
      "         [ 1.0000,  0.8129, -0.6142],\n",
      "         [-0.9942,  1.0000, -0.9997],\n",
      "         [ 0.8543, -0.0014, -0.9532]],\n",
      "\n",
      "        [[ 0.9913, -0.5334, -0.9988],\n",
      "         [ 0.9998, -0.9797, -0.8479],\n",
      "         [ 0.9998,  0.9973, -0.8168],\n",
      "         [ 0.9934,  0.6805, -0.9430]],\n",
      "\n",
      "        [[ 0.9919, -0.9317, -0.9506],\n",
      "         [ 1.0000, -0.2871, -0.7248],\n",
      "         [-0.9122,  0.9729, -0.9989],\n",
      "         [ 1.0000, -0.9907,  0.5449]],\n",
      "\n",
      "        [[-1.0000,  0.9516, -0.9998],\n",
      "         [-0.8675, -0.7986, -0.9954],\n",
      "         [ 0.8923,  0.9874, -0.9677],\n",
      "         [ 0.9638,  1.0000, -0.9992]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Initialize hiddens for update.\n",
    "# hiddens = torch.zeros(batch_size, seq_len, hidden_size)\n",
    "\n",
    "#Insert your code here\n",
    "hiddens_list = []\n",
    "for t in range(seq_len):\n",
    "    if t == 0:\n",
    "        # First timestep: h_t = tanh(U * x_t + b)\n",
    "        h_t = torch.tanh(torch.matmul(inputs[:, t, :], U) + b)\n",
    "        hiddens_list.append(h_t)\n",
    "    else:\n",
    "        # Subsequent timesteps: h_t = tanh(U * x_t + W * h_{t-1} + b)\n",
    "        h_t = torch.tanh(torch.matmul(inputs[:, t, :], U) + torch.matmul(hiddens_list[t-1], W) + b)\n",
    "        hiddens_list.append(h_t)\n",
    "\n",
    "hiddens = torch.stack(hiddens_list, dim=1)\n",
    "\n",
    "print(hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC3gaFvLtyUW"
   },
   "source": [
    "(3) In what follows, you need to write the code to compute the logits based on the last hidden state (``[batch_size, hidden_size]``) of hiddens.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[1 mark]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Psr-vrw8uCMN",
    "outputId": "93cb0e42-bc8c-46b5-cd3c-8be26642dd37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0300, -2.5415,  0.2041],\n",
      "        [-0.2528,  0.6185, -1.0825],\n",
      "        [-0.4119, -3.4979,  1.9043],\n",
      "        [-0.3032, -3.0811,  1.5632],\n",
      "        [ 0.2647, -1.0810, -0.1330],\n",
      "        [-0.2706, -2.7443,  1.2908],\n",
      "        [-2.3080,  1.6556, -0.5197],\n",
      "        [-0.2516, -3.4985,  1.8246]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.matmul(hiddens[:, -1, :], V) + c  # Use last hidden state\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DEJQQz9uKFb"
   },
   "source": [
    "(4) Write the code to compute the cross-entropy loss by comparing the logits to the labels. You can use PyTorch's built-in loss function.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[1 mark]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gyy7lbacuWPR",
    "outputId": "14b6c4ea-28af-4506-bf6a-fd901bc21271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8178, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Insert your code here\n",
    "loss = torch.nn.functional.cross_entropy(logits, random_labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVt-977bumET"
   },
   "source": [
    "(5) Next, you need to do back-propagation to compute the gradients of the loss w.r.t. the model parameters. You can use PyTorch's built-in method to compute the gradients.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "HSpeFroAu6oO"
   },
   "outputs": [],
   "source": [
    "#Insert your code here\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEkHFzOovBx1"
   },
   "source": [
    "(6) Finally, let assume that the learning rate $\\eta = 0.1$, you need to write the code to **manually** update the new model parameters using the SGD manner.\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2zFSfys8wMqs"
   },
   "outputs": [],
   "source": [
    "#Insert your code here\n",
    "eta = 0.1\n",
    "with torch.no_grad():\n",
    "    U -= eta * U.grad\n",
    "    W -= eta * W.grad\n",
    "    b -= eta * b.grad\n",
    "    V -= eta * V.grad\n",
    "    c -= eta * c.grad\n",
    "\n",
    "    # Zero gradients for next iteration\n",
    "    U.grad.zero_()\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    V.grad.zero_()\n",
    "    c.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAVPM0BnTdd4"
   },
   "source": [
    "## Section 2: Deep Learning for Sequential Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAgUSME4TsCP"
   },
   "source": [
    "### <font color=\"#0b486b\">Set random seeds</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00nhh4IRUcGX"
   },
   "source": [
    "We start with importing PyTorch and NumPy and setting random seeds for PyTorch and NumPy. You can use any seeds you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "O7XWUry0JXCc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6ZoWqunmUY7L"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_all(seed=1234)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VU1jS6SUl8q"
   },
   "source": [
    "## <font color=\"#0b486b\">Download and preprocess the data</font>\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQEzWmZjUulL"
   },
   "source": [
    "The dataset we use for this assignment is a question classification dataset for which the training set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
    "- abbreviation (ABBR),\n",
    "- entity (ENTY),\n",
    "- description (DESC),\n",
    "- human (HUM),\n",
    "- location (LOC) and\n",
    "- numeric (NUM).\n",
    "\n",
    "In this assignment, we will utilize a subset of this dataset, containing $2,000$ questions for training and validation. We will use 80% of those 2000 questions for trainning and the rest for validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOd49RTpUxxj"
   },
   "source": [
    "Preprocessing data is a crucial initial step in any machine learning or deep learning project. The *TextDataManager* class simplifies the process by providing functionalities to download and preprocess data specifically designed for the subsequent questions in this assignment. It is highly recommended to gain a comprehensive understanding of the class's functionality by **carefully reading** the content provided in the *TextDataManager* class before proceeding to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_C2fuJNzUhha"
   },
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"\n",
    "    This class manages and preprocesses a simple text dataset for a sentence classification task.\n",
    "\n",
    "    Attributes:\n",
    "        verbose (bool): Controls verbosity for printing information during data processing.\n",
    "        max_sentence_len (int): The maximum length of a sentence in the dataset.\n",
    "        str_questions (list): A list to store the string representations of the questions in the dataset.\n",
    "        str_labels (list): A list to store the string representations of the labels in the dataset.\n",
    "        numeral_labels (list): A list to store the numerical representations of the labels in the dataset.\n",
    "        numeral_data (list): A list to store the numerical representations of the questions in the dataset.\n",
    "        random_state (int): Seed value for random number generation to ensure reproducibility.\n",
    "            Set this value to a specific integer to reproduce the same random sequence every time. Defaults to 6789.\n",
    "        random (np.random.RandomState): Random number generator object initialized with the given random_state.\n",
    "            It is used for various random operations in the class.\n",
    "\n",
    "    Methods:\n",
    "        maybe_download(dir_name, file_name, url, verbose=True):\n",
    "            Downloads a file from a given URL if it does not exist in the specified directory.\n",
    "            The directory and file are created if they do not exist.\n",
    "\n",
    "        read_data(dir_name, file_names):\n",
    "            Reads data from files in a directory, preprocesses it, and computes the maximum sentence length.\n",
    "            Each file is expected to contain rows in the format \"<label>:<question>\".\n",
    "            The labels and questions are stored as string representations.\n",
    "\n",
    "        manipulate_data():\n",
    "            Performs data manipulation by tokenizing, numericalizing, and padding the text data.\n",
    "            The questions are tokenized and converted into numerical sequences using a tokenizer.\n",
    "            The sequences are padded or truncated to the maximum sequence length.\n",
    "\n",
    "        train_valid_test_split(train_ratio=0.9):\n",
    "            Splits the data into training, validation, and test sets based on a given ratio.\n",
    "            The data is randomly shuffled, and the specified ratio is used to determine the size of the training set.\n",
    "            The string questions, numerical data, and numerical labels are split accordingly.\n",
    "            TensorFlow `Dataset` objects are created for the training and validation sets.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=True, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = None\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose=True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "\n",
    "    def read_data(self, dir_name, file_names):\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str = row.split(\":\")\n",
    "                    label, question = row_str[0], row_str[1]\n",
    "                    question = question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len = len(self.str_questions[-1])\n",
    "\n",
    "        # turns labels into numbers\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes = le.classes_\n",
    "        self.num_classes = len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions and corresponding labels... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(self.str_labels[0:5])\n",
    "\n",
    "    def manipulate_data(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "        self.idx2word = {i:w for w,i in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "        token_ids = []\n",
    "        num_seqs = []\n",
    "        for text in self.str_questions:  # iterate over the list of text\n",
    "          text_seqs = self.tokenizer.tokenize(str(text))  # tokenize each text individually\n",
    "          # Convert tokens to IDs\n",
    "          token_ids = self.tokenizer.convert_tokens_to_ids(text_seqs)\n",
    "          # Convert token IDs to a tensor of indices using your word2idx mapping\n",
    "          seq_tensor = torch.LongTensor(token_ids)\n",
    "          num_seqs.append(seq_tensor)  # append the tensor for each sequence\n",
    "\n",
    "        # Pad the sequences and create a tensor\n",
    "        if num_seqs:\n",
    "          self.numeral_data = pad_sequence(num_seqs, batch_first=True)  # Pads to max length of the sequences\n",
    "          self.num_sentences, self.maxlen = self.numeral_data.shape\n",
    "\n",
    "    def train_valid_test_split(self, train_ratio=0.8, test_ratio = 0.1):\n",
    "        train_size = int(self.num_sentences*train_ratio) +1\n",
    "        test_size = int(self.num_sentences*test_ratio) +1\n",
    "        valid_size = self.num_sentences - (train_size + test_size)\n",
    "        data_indices = list(range(self.num_sentences))\n",
    "        random.shuffle(data_indices)\n",
    "        self.train_str_questions = [self.str_questions[i] for i in data_indices[:train_size]]\n",
    "        self.train_numeral_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_data = self.numeral_data[data_indices[:train_size]]\n",
    "        train_set_labels = self.numeral_labels[data_indices[:train_size]]\n",
    "        train_set_labels = torch.from_numpy(train_set_labels)\n",
    "        train_set = torch.utils.data.TensorDataset(train_set_data, train_set_labels)\n",
    "        self.test_str_questions = [self.str_questions[i] for i in data_indices[-test_size:]]\n",
    "        self.test_numeral_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_data = self.numeral_data[data_indices[-test_size:]]\n",
    "        test_set_labels = self.numeral_labels[data_indices[-test_size:]]\n",
    "        test_set_labels = torch.from_numpy(test_set_labels)\n",
    "        test_set = torch.utils.data.TensorDataset(test_set_data, test_set_labels)\n",
    "        self.valid_str_questions = [self.str_questions[i] for i in data_indices[train_size:-test_size]]\n",
    "        self.valid_numeral_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_data = self.numeral_data[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = self.numeral_labels[data_indices[train_size:-test_size]]\n",
    "        valid_set_labels = torch.from_numpy(valid_set_labels)\n",
    "        valid_set = torch.utils.data.TensorDataset(valid_set_data, valid_set_labels)\n",
    "        self.train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "        self.valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3npdESj6Vb_t",
    "outputId": "d93ace20-6e62-4aba-a239-1a29af70a279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloaded successfully train_2000.label\n",
      "\n",
      "Sample questions and corresponding labels... \n",
      "\n",
      "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
      "['DESC', 'ENTY', 'DESC', 'ENTY', 'ABBR']\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "DataManager.maybe_download(\"data\", \"train_2000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "\n",
    "dm = DataManager()\n",
    "dm.read_data(\"data/\", [\"train_2000.label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "EgrYZPmyVj60"
   },
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_test_split(train_ratio=0.8, test_ratio = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bH-U0sUMVnW-",
    "outputId": "5f43798d-2cbf-4b75-ba84-ab4e8ad1a8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 36]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in dm.train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frFf7-ehVvNM"
   },
   "source": [
    "## <font color=\"#0b486b\">Part 1: Using Word2Vect to transform texts to vectors </font>\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvyCbrXs7KeL",
    "outputId": "e13ef3ae-c078-4cf3-e314-fb1587fe1077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
      "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KZFrETlMVq7P"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JH-f1BJY9bw"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.1**</font>\n",
    "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
    "\n",
    "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b76lh_hZVyac",
    "outputId": "22a1cc18-fc57-4f19-d93d-3813cf50af3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word2vect = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GpbibKfdZB-S"
   },
   "outputs": [],
   "source": [
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        vector = model[word]\n",
    "    except:\n",
    "        vector = np.zeros(100)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TjRdyTg2_dN"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.2**</font>\n",
    "\n",
    "**Write the code for the function `get_sentence_vector(sentence, important_score=None, model= None)`. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
    "- $important\\_weight = softmax(important\\_score)$.\n",
    "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[T]\\times v[T]$ where $T$ is the length of the sentence and $v[i]$ is the vector representation of the $i-th$  word in this sentence.\n",
    "\n",
    "**Note that if `important_score=None` is set by default, your function should return the average of all representation vectors corresponding to set `important_score=[1,1,...,1]`.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MaPs4Hqa2_30"
   },
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, important_score=None, model= None):\n",
    "    #Insert your code here\n",
    "    words = sentence.split()\n",
    "    word_vectors = []\n",
    "\n",
    "    for word in words:\n",
    "        word_vector = get_word_vector(word, model)\n",
    "        word_vectors.append(word_vector)\n",
    "\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(100)\n",
    "\n",
    "    word_vectors = np.array(word_vectors)\n",
    "\n",
    "    if important_score is None:\n",
    "        # Return average of all word vectors\n",
    "        feature_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # Apply softmax to important_score to get weights\n",
    "        important_weight = torch.softmax(torch.tensor(important_score, dtype=torch.float32), dim=0).numpy()\n",
    "        # Weighted sum of word vectors\n",
    "        feature_vector = np.sum(word_vectors * important_weight.reshape(-1, 1), axis=0)\n",
    "\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu63GJ2c3IgZ"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.3**</font>\n",
    "\n",
    "**Write code to transform questions in *dm.train_str_questions* and *dm.valid_str_questions* to feature vectors. Note that after running the following cells, you must have $X\\_train$ and $X\\_valid$ which are two NumPy arrays of the feature vectors and $y\\_train$ and $y\\_valid$ which are two arrays of numeric labels (Hint: *dm.train_numeral_labels* and *dm.valid_numeral_labels*). You can add more lines to the following cells if necessary. In addition, you should decide the *important_score* by yourself. For example, the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SCsWIdZ3I4S",
    "outputId": "2641e1a1-e470-4878-f682-262ad89e3052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform training set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform training set to feature vectors...\")\n",
    "# Create important scores with decay factor 0.9\n",
    "X_train = []\n",
    "for sentence in dm.train_str_questions:\n",
    "    words = sentence.split()\n",
    "    important_score = [0.9 ** i for i in range(len(words))]\n",
    "    feature_vector = get_sentence_vector(sentence, important_score, word2vect)\n",
    "    X_train.append(feature_vector)\n",
    "X_train = np.array(X_train)\n",
    "y_train = dm.train_numeral_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2odznptZ3Nf_",
    "outputId": "2dbcc5a2-c077-4b1c-e141-f3dfa694220f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform validation set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform validation set to feature vectors...\")\n",
    "# Create important scores with decay factor 0.9\n",
    "X_valid = []\n",
    "for sentence in dm.valid_str_questions:\n",
    "    words = sentence.split()\n",
    "    important_score = [0.9 ** i for i in range(len(words))]\n",
    "    feature_vector = get_sentence_vector(sentence, important_score, word2vect)\n",
    "    X_valid.append(feature_vector)\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = dm.valid_numeral_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOp_gy3d3Reh"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.4**</font>\n",
    "\n",
    "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in scikit-learn to scale both training and validation sets to the range $(-1,1)$.**\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT47Q13c3VJV",
    "outputId": "9c8b3651-e957-49e5-e66b-18bc85770536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1601, 100) (198, 100)\n"
     ]
    }
   ],
   "source": [
    "#Insert your code\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "print(X_train.shape, X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcvKQyHO3ZJx"
   },
   "source": [
    "#### <font color=\"red\">**Question 1.5**</font>\n",
    "**Train a Logistic Regression model on the training set and then evaluate on the validation set.** You can use any classification metrics in `sklearn` for evaluation.\n",
    "<div style=\"text-align: right\"><font color=\"red\">[2 marks]</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "IQYE_rz-3b25",
    "outputId": "1a313e35-9b1e-438a-e93b-ee9b7462b05f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#Insert your code\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucJWNHMO3gXF",
    "outputId": "bc016b72-6b73-42a6-c23d-76d36d7c42f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9040\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.91      0.86      0.89        36\n",
      "           2       0.89      0.87      0.88        54\n",
      "           3       0.96      1.00      0.98        52\n",
      "           4       0.84      0.90      0.87        29\n",
      "           5       0.88      0.88      0.88        25\n",
      "\n",
      "    accuracy                           0.90       198\n",
      "   macro avg       0.91      0.83      0.86       198\n",
      "weighted avg       0.90      0.90      0.90       198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#Insert your code\n",
    "y_pred = lr_model.predict(X_valid)\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(metrics.classification_report(y_valid, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZs9Y0rl7gBH"
   },
   "source": [
    "We now declare the `BaseTrainer` class, which will be used later to train the subsequent deep learning models for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "yXlNQvGn7OEb"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BaseTrainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader):\n",
    "        self.model = model\n",
    "        self.criterion = criterion  #the loss function\n",
    "        self.optimizer = optimizer  #the optimizer\n",
    "        self.train_loader = train_loader  #the train loader\n",
    "        self.val_loader = val_loader  #the valid loader\n",
    "\n",
    "    #the function to train the model in many epochs\n",
    "    def fit(self, num_epochs):\n",
    "        self.num_batches = len(self.train_loader)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            train_loss, train_accuracy = self.train_one_epoch()\n",
    "            val_loss, val_accuracy = self.validate_one_epoch()\n",
    "            print(\n",
    "                f'{self.num_batches}/{self.num_batches} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy*100:.4f}% \\\n",
    "                - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy*100:.4f}%')\n",
    "\n",
    "    #train in one epoch, return the train_acc, train_loss\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_accuracy = correct / total\n",
    "        train_loss = running_loss / self.num_batches\n",
    "        return train_loss, train_accuracy\n",
    "\n",
    "    #evaluate on a loader and return the loss and accuracy\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        loss = loss / len(self.val_loader)\n",
    "        return loss, accuracy\n",
    "\n",
    "    #return the val_acc, val_loss, be called at the end of each epoch\n",
    "    def validate_one_epoch(self):\n",
    "      val_loss, val_accuracy = self.evaluate(self.val_loader)\n",
    "      return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GWOQN_S7p4V"
   },
   "source": [
    "## <font color=\"#0b486b\">Part 2: Text CNN for sequence modeling and neural embedding </font>\n",
    "\n",
    "<div style=\"text-align: right\"><font color=\"red; font-weight:bold\">[Total marks for this part: 10 marks]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJub3Fwm7vC7"
   },
   "source": [
    "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN that you need to construct.**\n",
    "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
    "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
    "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)).\n",
    "  - `data_manager`: the data manager to store information of the dataset.\n",
    "- The detail of the computational process is as follows:\n",
    "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size, seq\\_len, embed\\_size]$ as $e$.\n",
    "  - We feed $e$ to three *Conv1D* layers, each of which has $state\\_size$ filters, activation= $relu$, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size, state\\_size, output\\_size]$. Moreover, you need to apply *Conv1D* to the $seq\\_len$ dimension.\n",
    "  - We then apply *GlobalMaxPool1D()* (reference [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html#torch.nn.functional.max_pool1d)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
    "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$ with the shape $\\left[batch\\_size, 3\\times state\\_size\\right]$. Note that you need to specify the axis to concatenate.\n",
    "  - We finally build up one dense layer $\\left[3\\times state\\_size, num\\_classes\\right]$  on the top of $h$ for classification.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "lEnG-BGJ239k"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#You can modify the code if you want but need to keep the skeleton\n",
    "class TextCNN(torch.nn.Module):\n",
    "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
    "        super().__init__()\n",
    "        self.data_manager = data_manager\n",
    "        self.embed_size = embed_size\n",
    "        self.state_size = state_size\n",
    "        #declare the necessary layers here\n",
    "        self.embed = nn.Embedding(self.data_manager.vocab_size, self.embed_size)\n",
    "        self.conv1d_1 = nn.Conv1d(self.embed_size, self.state_size, kernel_size=3)\n",
    "        self.conv1d_2 = nn.Conv1d(self.embed_size, self.state_size, kernel_size=5)\n",
    "        self.conv1d_3 = nn.Conv1d(self.embed_size, self.state_size, kernel_size=7)\n",
    "        self.fc = nn.Linear(state_size*3, self.data_manager.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        #permute x before applying Conv1D\n",
    "        e= e.permute(0,2,1)\n",
    "\n",
    "        #applying Conv1D\n",
    "        h1 = F.relu(self.conv1d_1(e))\n",
    "        h2 = F.relu(self.conv1d_2(e))\n",
    "        h3 = F.relu(self.conv1d_3(e))\n",
    "\n",
    "        #apply GlobalMaxPool\n",
    "        h1 = F.max_pool1d(h1, kernel_size=h1.size(2))\n",
    "        h2 = F.max_pool1d(h2, kernel_size=h2.size(2))\n",
    "        h3 = F.max_pool1d(h3, kernel_size=h3.size(2))\n",
    "\n",
    "        h = torch.cat([h1.squeeze(2), h2.squeeze(2), h3.squeeze(2)], dim=1)\n",
    "        h = self.fc(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-PN6KmIFw6K"
   },
   "source": [
    "We declare `text_cnn` and train on several epochs (e.g., `50 epochs`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrL5oM82BR3_",
    "outputId": "0cc374b0-28a4-4fbe-a3eb-d429ac507168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 - train_loss: 1.6139 - train_accuracy: 36.8520%                 - val_loss: 0.5991 - val_accuracy: 71.7172%\n",
      "Epoch 2/50\n",
      "26/26 - train_loss: 0.8245 - train_accuracy: 80.3248%                 - val_loss: 0.3028 - val_accuracy: 80.3030%\n",
      "Epoch 3/50\n",
      "26/26 - train_loss: 0.4867 - train_accuracy: 89.7564%                 - val_loss: 0.1956 - val_accuracy: 90.4040%\n",
      "Epoch 4/50\n",
      "26/26 - train_loss: 0.2666 - train_accuracy: 94.4410%                 - val_loss: 0.1611 - val_accuracy: 90.9091%\n",
      "Epoch 5/50\n",
      "26/26 - train_loss: 0.1737 - train_accuracy: 97.0643%                 - val_loss: 0.1311 - val_accuracy: 92.9293%\n",
      "Epoch 6/50\n",
      "26/26 - train_loss: 0.1166 - train_accuracy: 98.8757%                 - val_loss: 0.1156 - val_accuracy: 93.4343%\n",
      "Epoch 7/50\n",
      "26/26 - train_loss: 0.0937 - train_accuracy: 99.5628%                 - val_loss: 0.0813 - val_accuracy: 93.9394%\n",
      "Epoch 8/50\n",
      "26/26 - train_loss: 0.0773 - train_accuracy: 98.5009%                 - val_loss: 0.0964 - val_accuracy: 94.9495%\n",
      "Epoch 9/50\n",
      "26/26 - train_loss: 0.0471 - train_accuracy: 99.8751%                 - val_loss: 0.0804 - val_accuracy: 93.9394%\n",
      "Epoch 10/50\n",
      "26/26 - train_loss: 0.0364 - train_accuracy: 100.0000%                 - val_loss: 0.0670 - val_accuracy: 93.9394%\n",
      "Epoch 11/50\n",
      "26/26 - train_loss: 0.0301 - train_accuracy: 100.0000%                 - val_loss: 0.0722 - val_accuracy: 94.4444%\n",
      "Epoch 12/50\n",
      "26/26 - train_loss: 0.0241 - train_accuracy: 100.0000%                 - val_loss: 0.0625 - val_accuracy: 94.9495%\n",
      "Epoch 13/50\n",
      "26/26 - train_loss: 0.0193 - train_accuracy: 100.0000%                 - val_loss: 0.0480 - val_accuracy: 94.9495%\n",
      "Epoch 14/50\n",
      "26/26 - train_loss: 0.0164 - train_accuracy: 100.0000%                 - val_loss: 0.0545 - val_accuracy: 95.4545%\n",
      "Epoch 15/50\n",
      "26/26 - train_loss: 0.0139 - train_accuracy: 100.0000%                 - val_loss: 0.0430 - val_accuracy: 95.4545%\n",
      "Epoch 16/50\n",
      "26/26 - train_loss: 0.0119 - train_accuracy: 100.0000%                 - val_loss: 0.0452 - val_accuracy: 95.9596%\n",
      "Epoch 17/50\n",
      "26/26 - train_loss: 0.0103 - train_accuracy: 100.0000%                 - val_loss: 0.0380 - val_accuracy: 95.4545%\n",
      "Epoch 18/50\n",
      "26/26 - train_loss: 0.0091 - train_accuracy: 100.0000%                 - val_loss: 0.0358 - val_accuracy: 95.4545%\n",
      "Epoch 19/50\n",
      "26/26 - train_loss: 0.0079 - train_accuracy: 100.0000%                 - val_loss: 0.0342 - val_accuracy: 95.9596%\n",
      "Epoch 20/50\n",
      "26/26 - train_loss: 0.0074 - train_accuracy: 100.0000%                 - val_loss: 0.0311 - val_accuracy: 95.9596%\n",
      "Epoch 21/50\n",
      "26/26 - train_loss: 0.0065 - train_accuracy: 100.0000%                 - val_loss: 0.0280 - val_accuracy: 95.9596%\n",
      "Epoch 22/50\n",
      "26/26 - train_loss: 0.0057 - train_accuracy: 100.0000%                 - val_loss: 0.0275 - val_accuracy: 95.9596%\n",
      "Epoch 23/50\n",
      "26/26 - train_loss: 0.0055 - train_accuracy: 100.0000%                 - val_loss: 0.0254 - val_accuracy: 95.9596%\n",
      "Epoch 24/50\n",
      "26/26 - train_loss: 0.0052 - train_accuracy: 100.0000%                 - val_loss: 0.0248 - val_accuracy: 95.9596%\n",
      "Epoch 25/50\n",
      "26/26 - train_loss: 0.0045 - train_accuracy: 100.0000%                 - val_loss: 0.0191 - val_accuracy: 95.9596%\n",
      "Epoch 26/50\n",
      "26/26 - train_loss: 0.0042 - train_accuracy: 100.0000%                 - val_loss: 0.0180 - val_accuracy: 95.9596%\n",
      "Epoch 27/50\n",
      "26/26 - train_loss: 0.0038 - train_accuracy: 100.0000%                 - val_loss: 0.0180 - val_accuracy: 95.9596%\n",
      "Epoch 28/50\n",
      "26/26 - train_loss: 0.0033 - train_accuracy: 100.0000%                 - val_loss: 0.0174 - val_accuracy: 95.9596%\n",
      "Epoch 29/50\n",
      "26/26 - train_loss: 0.0031 - train_accuracy: 100.0000%                 - val_loss: 0.0169 - val_accuracy: 95.9596%\n",
      "Epoch 30/50\n",
      "26/26 - train_loss: 0.0029 - train_accuracy: 100.0000%                 - val_loss: 0.0157 - val_accuracy: 96.4646%\n",
      "Epoch 31/50\n",
      "26/26 - train_loss: 0.0026 - train_accuracy: 100.0000%                 - val_loss: 0.0154 - val_accuracy: 96.4646%\n",
      "Epoch 32/50\n",
      "26/26 - train_loss: 0.0025 - train_accuracy: 100.0000%                 - val_loss: 0.0142 - val_accuracy: 96.4646%\n",
      "Epoch 33/50\n",
      "26/26 - train_loss: 0.0023 - train_accuracy: 100.0000%                 - val_loss: 0.0134 - val_accuracy: 96.4646%\n",
      "Epoch 34/50\n",
      "26/26 - train_loss: 0.0022 - train_accuracy: 100.0000%                 - val_loss: 0.0124 - val_accuracy: 95.9596%\n",
      "Epoch 35/50\n",
      "26/26 - train_loss: 0.0020 - train_accuracy: 100.0000%                 - val_loss: 0.0115 - val_accuracy: 96.4646%\n",
      "Epoch 36/50\n",
      "26/26 - train_loss: 0.0019 - train_accuracy: 100.0000%                 - val_loss: 0.0112 - val_accuracy: 96.4646%\n",
      "Epoch 37/50\n",
      "26/26 - train_loss: 0.0018 - train_accuracy: 100.0000%                 - val_loss: 0.0107 - val_accuracy: 96.4646%\n",
      "Epoch 38/50\n",
      "26/26 - train_loss: 0.0017 - train_accuracy: 100.0000%                 - val_loss: 0.0102 - val_accuracy: 96.4646%\n",
      "Epoch 39/50\n",
      "26/26 - train_loss: 0.0016 - train_accuracy: 100.0000%                 - val_loss: 0.0100 - val_accuracy: 96.4646%\n",
      "Epoch 40/50\n",
      "26/26 - train_loss: 0.0015 - train_accuracy: 100.0000%                 - val_loss: 0.0095 - val_accuracy: 96.4646%\n",
      "Epoch 41/50\n",
      "26/26 - train_loss: 0.0015 - train_accuracy: 100.0000%                 - val_loss: 0.0090 - val_accuracy: 96.4646%\n",
      "Epoch 42/50\n",
      "26/26 - train_loss: 0.0014 - train_accuracy: 100.0000%                 - val_loss: 0.0087 - val_accuracy: 96.4646%\n",
      "Epoch 43/50\n",
      "26/26 - train_loss: 0.0013 - train_accuracy: 100.0000%                 - val_loss: 0.0084 - val_accuracy: 96.4646%\n",
      "Epoch 44/50\n",
      "26/26 - train_loss: 0.0014 - train_accuracy: 100.0000%                 - val_loss: 0.0081 - val_accuracy: 96.4646%\n",
      "Epoch 45/50\n",
      "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.0076 - val_accuracy: 95.9596%\n",
      "Epoch 46/50\n",
      "26/26 - train_loss: 0.0011 - train_accuracy: 100.0000%                 - val_loss: 0.0077 - val_accuracy: 96.4646%\n",
      "Epoch 47/50\n",
      "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.0073 - val_accuracy: 96.4646%\n",
      "Epoch 48/50\n",
      "26/26 - train_loss: 0.0012 - train_accuracy: 100.0000%                 - val_loss: 0.0074 - val_accuracy: 96.4646%\n",
      "Epoch 49/50\n",
      "26/26 - train_loss: 0.0010 - train_accuracy: 100.0000%                 - val_loss: 0.0079 - val_accuracy: 96.4646%\n",
      "Epoch 50/50\n",
      "26/26 - train_loss: 0.0009 - train_accuracy: 100.0000%                 - val_loss: 0.0064 - val_accuracy: 96.4646%\n"
     ]
    }
   ],
   "source": [
    "text_cnn = TextCNN(data_manager=dm).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_cnn.parameters(), lr=0.001)\n",
    "trainer = BaseTrainer(model=text_cnn, criterion=criterion, optimizer=optimizer, train_loader=dm.train_loader, val_loader=dm.valid_loader)\n",
    "trainer.fit(num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrjO5iT6F_Li"
   },
   "source": [
    "We evaluate the trained model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnHiWCl7BtJC",
    "outputId": "d87a0aeb-d910-4578-95ee-08e2124a23b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.2677 - test_accuracy: 95.5224%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = trainer.evaluate(dm.test_loader)\n",
    "print(f'test_loss: {test_loss:.4f} - test_accuracy: {test_acc*100:.4f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
