{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac88dce",
   "metadata": {},
   "source": [
    "> Fullname: Yiming Zhang \n",
    "> Student ID: 35224436"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a65c25",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Deep Neural Networks (DNN) </span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 25 points]<span></div>\n",
    "\n",
    "The second part of this assignment is to demonstrate your basis knowledge in deep learning that you have acquired from the lectures and tutorials materials. Most of the contents in this assignment are drawn from **the tutorials covered from weeks 1 to 2**. Going through these materials before attempting this assignment is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd76975",
   "metadata": {},
   "source": [
    "In the second part of this assignment, you are going to work with the FashionMNIST dataset for the image recognition task. It has the exact same format as MNIST (70,000 grayscale images of 28 Ã— 28 pixels each with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse, and the problem is significantly more challenging than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c65ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd666d6bdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e718e2b",
   "metadata": {},
   "source": [
    "**Load the Fashion MNIST using `torchvision`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30660e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([60000])\n",
      "torch.Size([10000, 28, 28]) torch.Size([10000])\n",
      "torch.Size([60000, 784]) torch.Size([60000])\n",
      "torch.Size([10000, 784]) torch.Size([10000])\n",
      "Number of training samples: 60000\n",
      "Number of training samples: 54000\n",
      "Number of validation samples: 6000\n",
      "54000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset_orgin = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(train_dataset_orgin.data.shape, train_dataset_orgin.targets.shape)\n",
    "print(test_dataset.data.shape, test_dataset.targets.shape)\n",
    "\n",
    "# Flatten the data\n",
    "train_dataset_orgin.data = train_dataset_orgin.data.reshape(-1, 28*28)\n",
    "test_dataset.data = test_dataset.data.reshape(-1, 28*28)\n",
    "\n",
    "print(train_dataset_orgin.data.shape, train_dataset_orgin.targets.shape)\n",
    "print(test_dataset.data.shape, test_dataset.targets.shape)\n",
    "\n",
    "N = len(train_dataset_orgin)\n",
    "print(f\"Number of training samples: {N}\")\n",
    "N_train = int(0.9*N)\n",
    "N_val = N - N_train\n",
    "print(f\"Number of training samples: {N_train}\")\n",
    "print(f\"Number of validation samples: {N_val}\")\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset_orgin, [N_train, N_val])\n",
    "\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94102335",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.1:**</span> Write the code to visualize a mini-batch in `train_loader` including its images and labels.\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94855678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e65bd",
   "metadata": {},
   "source": [
    "####<span style=\"color:red\">**Question 2.2:**</span> Write the code for the feed-forward neural net using PyTorch\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b41d4b",
   "metadata": {},
   "source": [
    "We now develop a feed-forward neural network with the architecture $784 \\rightarrow 40(ReLU) \\rightarrow 30(ReLU) \\rightarrow 10(softmax)$. You can choose your own way to implement your network and an optimizer of interest. You should train model in $50$ epochs and evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d6e2f3",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.3:**</span> Tuning hyper-parameters with grid search\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b55b69",
   "metadata": {},
   "source": [
    "Assume that you need to tune the number of neurons on the first and second hidden layers $n_1 \\in \\{20, 40\\}$, $n_2 \\in \\{20, 40\\}$  and the used activation function  $act \\in \\{sigmoid, tanh, relu\\}$. The network has the architecture pattern $784 \\rightarrow n_1 (act) \\rightarrow n_2(act) \\rightarrow 10(softmax)$ where $n_1, n_2$, and $act$ are in their grides. Write the code to tune the hyper-parameters $n_1, n_2$, and $act$. Note that you can freely choose the optimizer and learning rate of interest for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f0222",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.4:**</span> Implement the loss with the form: $loss(p,y)=CE(1_{y},p)+\\lambda H(p)$ where $H(p)=-\\sum_{i=1}^{M}p_{i}\\log p_{i}$ is the entropy of $p$, $p$ is the prediction probabilities of a data point $x$ with the ground-truth label $y$, $1_y$ is an one-hot label, and $\\lambda >0$ is a trade-off parameter. Set $\\lambda = 0.1$ to train a model.\n",
    "\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4def064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88317f",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 2.5:**</span> Experimenting with **sharpness-aware minimization** technique\n",
    "<div style=\"text-align: right\"> <span style=\"color:red\">[5 points]</span> </div>\n",
    "\n",
    "Sharpness-aware minimization (SAM) (i.e., [link for main paper](https://openreview.net/pdf?id=6Tm1mposlrM) from Google Deepmind) is a simple yet but efficient technique to improve the generalization ability of deep learning models on unseen data examples. In your research or your work, you might potentially use this idea. Your task is to read the paper and implement *Sharpness-aware minimization (SAM)*. Finally, you need to apply SAM to the best architecture found in **Question 2.3**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
