{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/FIT5215 Deep learning' # Adjust the path if necessary\n",
        "\n",
        "# Define the destination directory in Colab\n",
        "destination_dir = '/content'\n",
        "\n",
        "# Define the files to copy\n",
        "files_to_copy = ['Animals_Dataset.zip', 'fit-5215-object-detection-s-2-2025.zip']\n",
        "\n",
        "# Copy the files\n",
        "for file_name in files_to_copy:\n",
        "    source_path = os.path.join(source_dir, file_name)\n",
        "    destination_path = os.path.join(destination_dir, file_name)\n",
        "    if os.path.exists(source_path):\n",
        "        !cp \"{source_path}\" \"{destination_path}\"\n",
        "        print(f\"Copied {file_name} to {destination_dir}\")\n",
        "    else:\n",
        "        print(f\"File not found: {source_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KWMTcbvPIsLH",
        "outputId": "2c258503-b8ee-4f1f-e951-8ff42d318de4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Copied Animals_Dataset.zip to /content\n",
            "Copied fit-5215-object-detection-s-2-2025.zip to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Animals_Dataset.zip\n",
        "!unzip -q fit-5215-object-detection-s-2-2025.zip"
      ],
      "metadata": {
        "id": "Hf9oMVxeJDCB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# 设置随机种子\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# 动态获取类别名称的函数\n",
        "def get_class_names(train_dir):\n",
        "    \"\"\"从训练数据目录动态获取类别名称\"\"\"\n",
        "    if not os.path.exists(train_dir):\n",
        "        raise ValueError(f\"训练数据目录不存在: {train_dir}\")\n",
        "\n",
        "    # 获取所有子目录名称作为类别名称\n",
        "    class_names = []\n",
        "    for item in os.listdir(train_dir):\n",
        "        item_path = os.path.join(train_dir, item)\n",
        "        # 只包含目录，排除文件\n",
        "        if os.path.isdir(item_path):\n",
        "            class_names.append(item)\n",
        "\n",
        "    # 排序以确保一致性\n",
        "    class_names.sort()\n",
        "    print(f\"发现 {len(class_names)} 个类别: {class_names}\")\n",
        "    return class_names\n",
        "\n",
        "# ==================== 高级数据增强 ====================\n",
        "class MixUp:\n",
        "    \"\"\"MixUp数据增强：混合两个样本及其标签\"\"\"\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, images, labels):\n",
        "        batch_size = images.size(0)\n",
        "        if self.alpha > 0:\n",
        "            lam = np.random.beta(self.alpha, self.alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "\n",
        "        index = torch.randperm(batch_size).to(images.device)\n",
        "        mixed_images = lam * images + (1 - lam) * images[index, :]\n",
        "        labels_a, labels_b = labels, labels[index]\n",
        "        return mixed_images, labels_a, labels_b, lam\n",
        "\n",
        "class CutMix:\n",
        "    \"\"\"CutMix数据增强：剪切并粘贴图片区域\"\"\"\n",
        "    def __init__(self, beta=1.0, prob=0.5):\n",
        "        self.beta = beta\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, images, labels):\n",
        "        batch_size = images.size(0)\n",
        "        if np.random.rand() > self.prob:\n",
        "            return images, labels, labels, 1.0\n",
        "\n",
        "        lam = np.random.beta(self.beta, self.beta)\n",
        "        rand_index = torch.randperm(batch_size).to(images.device)\n",
        "\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(images.size(), lam)\n",
        "        images[:, :, bbx1:bbx2, bby1:bby2] = images[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size()[-1] * images.size()[-2]))\n",
        "        labels_a, labels_b = labels, labels[rand_index]\n",
        "        return images, labels_a, labels_b, lam\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[2]\n",
        "        H = size[3]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = int(W * cut_rat)\n",
        "        cut_h = int(H * cut_rat)\n",
        "\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# 高级数据增强管道\n",
        "def get_advanced_train_transform():\n",
        "    return transforms.Compose([\n",
        "        # 随机选择不同的缩放策略\n",
        "        transforms.RandomChoice([\n",
        "            transforms.Resize((64, 64)),   # 25% 概率使用测试集大小\n",
        "            transforms.Resize((96, 96)),\n",
        "            transforms.Resize((128, 128)),\n",
        "            transforms.Resize((224, 224)),  # 25% 概率使用标准大小\n",
        "        ]),\n",
        "        transforms.Resize((224, 224)),  # 统一到224x224\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        # 更强的数据增强\n",
        "        transforms.RandomApply([\n",
        "            transforms.RandomRotation(30),\n",
        "        ], p=0.5),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.2),\n",
        "        ], p=0.5),\n",
        "        transforms.RandomApply([\n",
        "            transforms.GaussianBlur(kernel_size=3),\n",
        "        ], p=0.3),\n",
        "        transforms.RandomApply([\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
        "        ], p=0.5),\n",
        "        # 随机擦除\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
        "    ])\n",
        "\n",
        "# ==================== 模型集成 ====================\n",
        "class EnsembleModel(nn.Module):\n",
        "    \"\"\"集成多个模型的预测\"\"\"\n",
        "    def __init__(self, num_classes=20):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "\n",
        "        # 模型1: ResNet50\n",
        "        self.model1 = models.resnet50(pretrained=True)\n",
        "        num_features1 = self.model1.fc.in_features\n",
        "        self.model1.fc = nn.Sequential(\n",
        "            nn.Linear(num_features1, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # 模型2: EfficientNet-B1\n",
        "        self.model2 = models.efficientnet_b1(pretrained=True)\n",
        "        num_features2 = self.model2.classifier[1].in_features\n",
        "        self.model2.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(num_features2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # 模型3: DenseNet121\n",
        "        self.model3 = models.densenet121(pretrained=True)\n",
        "        num_features3 = self.model3.classifier.in_features\n",
        "        self.model3.classifier = nn.Sequential(\n",
        "            nn.Linear(num_features3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # 可学习的权重\n",
        "        self.weights = nn.Parameter(torch.ones(3) / 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.model1(x)\n",
        "        out2 = self.model2(x)\n",
        "        out3 = self.model3(x)\n",
        "\n",
        "        # 加权平均\n",
        "        w = F.softmax(self.weights, dim=0)\n",
        "        output = w[0] * out1 + w[1] * out2 + w[2] * out3\n",
        "        return output\n",
        "\n",
        "# ==================== 单模型但更强大 ====================\n",
        "class PowerfulSingleModel(nn.Module):\n",
        "    def __init__(self, num_classes=20, model_name='efficientnet_b2'):\n",
        "        super(PowerfulSingleModel, self).__init__()\n",
        "\n",
        "        if model_name == 'efficientnet_b2':\n",
        "            self.base = models.efficientnet_b2(pretrained=True)\n",
        "            # 解冻更多层\n",
        "            for name, param in self.base.named_parameters():\n",
        "                if 'features.5' in name or 'features.6' in name or 'features.7' in name:\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            num_features = self.base.classifier[1].in_features\n",
        "            self.base.classifier = nn.Sequential(\n",
        "                nn.Dropout(0.4),\n",
        "                nn.Linear(num_features, 1024),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(1024),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(1024, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(512),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(512, num_classes)\n",
        "            )\n",
        "\n",
        "        elif model_name == 'convnext_tiny':\n",
        "            self.base = models.convnext_tiny(pretrained=True)\n",
        "            # ConvNeXt在小图片上表现很好\n",
        "            for param in self.base.features[:-2].parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            num_features = self.base.classifier[2].in_features\n",
        "            self.base.classifier[2] = nn.Sequential(\n",
        "                nn.Linear(num_features, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(512, num_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "# ==================== 改进的数据集类 ====================\n",
        "class AdvancedDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels=None, transform=None,\n",
        "                 training=False, mixup_alpha=0.2, cutmix_prob=0.5):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.training = training\n",
        "        self.mixup = MixUp(alpha=mixup_alpha) if training else None\n",
        "        self.cutmix = CutMix(prob=cutmix_prob) if training else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[idx]\n",
        "            return image, label\n",
        "        else:\n",
        "            return image, 0\n",
        "\n",
        "# ============ OVA Loss ==================\n",
        "class OVALoss(nn.Module):\n",
        "    \"\"\"\n",
        "    One-vs-All multi-class loss implemented via BCEWithLogits.\n",
        "    - 接受：索引标签 [B] 或 软标签 [B, C]\n",
        "    - label_smoothing: 对正/负类做轻度平滑（可选）\n",
        "    - reduction='mean'：对类和样本取均值（与CE同量级）\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int, label_smoothing: float = 0.0, reduction: str = \"mean\"):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.eps = float(label_smoothing)\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def _to_one_hot(self, target: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
        "        # target: [B] (long) -> [B, C] (float)\n",
        "        return F.one_hot(target, num_classes=num_classes).float()\n",
        "\n",
        "    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        logits: [B, C] (未过sigmoid)\n",
        "        target: [B] (索引) 或 [B, C] (软标签/MixUp)\n",
        "        \"\"\"\n",
        "        B, C = logits.shape\n",
        "        if target.dim() == 1:\n",
        "            target = self._to_one_hot(target, C)  # [B,C]\n",
        "\n",
        "        # Label smoothing: 正类=1-eps；负类=eps/(C-1)\n",
        "        if self.eps > 0.0:\n",
        "            pos = 1.0 - self.eps\n",
        "            neg = self.eps / (C - 1)\n",
        "            target = target * (pos - neg) + neg\n",
        "\n",
        "        # OVA 等价于对每个类做二分类 BCE\n",
        "        # reduction='none' 得到 [B,C]，再按类平均，最后按样本平均\n",
        "        loss_mat = F.binary_cross_entropy_with_logits(logits, target, reduction=\"none\")\n",
        "        loss = loss_mat.mean(dim=1)  # 先对类取平均\n",
        "        if self.reduction == \"mean\":\n",
        "            return loss.mean()        # 再对样本取平均\n",
        "        elif self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss  # 'none'\n",
        "\n",
        "\n",
        "# ==================== 训练技巧 ====================\n",
        "def train_epoch_advanced(model, train_loader, criterion, optimizer, device, mixup=None, cutmix=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # 应用MixUp或CutMix\n",
        "        if mixup and random.random() < 0.5:\n",
        "            images, labels_a, labels_b, lam = mixup(images, labels)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "        elif cutmix:\n",
        "            images, labels_a, labels_b, lam = cutmix(images, labels)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "        else:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # 梯度裁剪\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def train_epoch_advanced_OVA(model, train_loader, criterion, optimizer, device, mixup=None, cutmix=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        use_mix = (mixup is not None) and (random.random() < 0.5)\n",
        "        if use_mix:\n",
        "            images, labels_a, labels_b, lam = mixup(images, labels)\n",
        "            outputs = model(images)\n",
        "            # 生成软目标（one-hot 后按 lam 混合） -> [B,C]\n",
        "            num_classes = outputs.size(1)\n",
        "            ta = F.one_hot(labels_a, num_classes=num_classes).float()\n",
        "            tb = F.one_hot(labels_b, num_classes=num_classes).float()\n",
        "            soft_target = lam * ta + (1.0 - lam) * tb\n",
        "            loss = criterion(outputs, soft_target)  # OVA 支持软标签\n",
        "        else:\n",
        "            if cutmix is not None:\n",
        "                images, labels_a, labels_b, lam = cutmix(images, labels)\n",
        "                outputs = model(images)\n",
        "                num_classes = outputs.size(1)\n",
        "                ta = F.one_hot(labels_a, num_classes=num_classes).float()\n",
        "                tb = F.one_hot(labels_b, num_classes=num_classes).float()\n",
        "                soft_target = lam * ta + (1.0 - lam) * tb\n",
        "                loss = criterion(outputs, soft_target)\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)  # 索引标签\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # 精度用 argmax(logits)。sigmoid 单调，argmax(logit)==argmax(sigmoid)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "# ==================== 知识蒸馏 ====================\n",
        "class KnowledgeDistillationLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7, temperature=4):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, student_outputs, teacher_outputs, labels):\n",
        "        # 硬标签损失\n",
        "        hard_loss = self.ce_loss(student_outputs, labels)\n",
        "\n",
        "        # 软标签损失（知识蒸馏）\n",
        "        soft_loss = F.kl_div(\n",
        "            F.log_softmax(student_outputs / self.temperature, dim=1),\n",
        "            F.softmax(teacher_outputs / self.temperature, dim=1),\n",
        "            reduction='batchmean'\n",
        "        ) * (self.temperature ** 2)\n",
        "\n",
        "        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "\n",
        "# ==================== 高级预测策略 ====================\n",
        "def predict_with_advanced_tta(model, test_image_paths, device, class_names, n_tta=10):\n",
        "    \"\"\"\n",
        "    高级测试时增强\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    tta_transforms = []\n",
        "    for i in range(n_tta):\n",
        "        if i == 0:\n",
        "            # 原始\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        elif i < 3:\n",
        "            # 不同的resize策略\n",
        "            size = [256, 288][i-1]\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((size, size)),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        elif i < 5:\n",
        "            # 水平翻转\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.RandomHorizontalFlip(p=1.0),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            # 随机裁剪\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        tta_transforms.append(transform)\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    for img_path in tqdm(test_image_paths, desc=\"Advanced TTA\"):\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for transform in tta_transforms:\n",
        "                img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "                output = model(img_tensor)\n",
        "                prob = F.softmax(output, dim=1)\n",
        "                predictions.append(prob.cpu().numpy())\n",
        "\n",
        "        # 加权平均（给原始图片更高权重）\n",
        "        weights = [2.0] + [1.0] * (n_tta - 1)\n",
        "        weights = np.array(weights) / sum(weights)\n",
        "        avg_prediction = np.average(predictions, axis=0, weights=weights)\n",
        "\n",
        "        final_pred = np.argmax(avg_prediction)\n",
        "        all_predictions.append(final_pred)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'ID': range(len(all_predictions)),\n",
        "        'Label': [class_names[pred] for pred in all_predictions]\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# ==================== 主训练流程 ====================\n",
        "def train_advanced_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 超参数\n",
        "    batch_size = 32  # 减小batch size以适应更大的模型\n",
        "    num_epochs = 40\n",
        "    learning_rate = 0.0005\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    # 数据路径\n",
        "    train_dir = '/content/FIT5215_Dataset'\n",
        "    test_dir = '/content/test_set/official_test'\n",
        "\n",
        "    # 动态获取类别名称\n",
        "    class_names = get_class_names(train_dir)\n",
        "\n",
        "    # 加载数据\n",
        "    train_image_paths = []\n",
        "    train_labels = []\n",
        "\n",
        "    if os.path.exists(train_dir):\n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            class_dir = os.path.join(train_dir, class_name)\n",
        "            if os.path.exists(class_dir):\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        train_image_paths.append(os.path.join(class_dir, img_name))\n",
        "                        train_labels.append(class_idx)\n",
        "\n",
        "    # 加载测试数据\n",
        "    test_image_paths = []\n",
        "    if os.path.exists(test_dir):\n",
        "        test_files = sorted(os.listdir(test_dir),\n",
        "                          key=lambda x: int(os.path.splitext(x)[0]) if os.path.splitext(x)[0].isdigit() else x)\n",
        "        test_image_paths = [os.path.join(test_dir, f) for f in test_files\n",
        "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    print(f\"训练数据: {len(train_image_paths)} 张\")\n",
        "    print(f\"测试数据: {len(test_image_paths)} 张\")\n",
        "\n",
        "    # 数据划分\n",
        "    train_paths, val_paths, train_labs, val_labs = train_test_split(\n",
        "        train_image_paths, train_labels, test_size=0.15, random_state=42, stratify=train_labels\n",
        "    )\n",
        "\n",
        "    # 创建数据集\n",
        "    train_dataset = AdvancedDataset(\n",
        "        train_paths, train_labs,\n",
        "        transform=get_advanced_train_transform(),\n",
        "        training=True\n",
        "    )\n",
        "    val_dataset = AdvancedDataset(\n",
        "        val_paths, val_labs,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 初始化模型\n",
        "    model = PowerfulSingleModel(num_classes=20, model_name='convnext_tiny')\n",
        "    # 或使用集成模型（更慢但可能更准）\n",
        "    # model = EnsembleModel(num_classes=20)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 损失函数和优化器\n",
        "    # criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    criterion = OVALoss(num_classes=20, label_smoothing=0.05)  # smoothing 可调或设 0\n",
        "\n",
        "    # 分层学习率\n",
        "    param_groups = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if 'classifier' in name or 'fc' in name:\n",
        "                param_groups.append({'params': param, 'lr': learning_rate})\n",
        "            else:\n",
        "                param_groups.append({'params': param, 'lr': learning_rate * 0.1})\n",
        "\n",
        "    optimizer = optim.AdamW(param_groups, weight_decay=weight_decay)\n",
        "\n",
        "    # 学习率调度\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=learning_rate * 10,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    # MixUp和CutMix\n",
        "    mixup = MixUp(alpha=0.2)\n",
        "    cutmix = CutMix(beta=1.0, prob=0.5)\n",
        "\n",
        "    # 训练\n",
        "    best_val_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch_advanced_OVA(\n",
        "            model, train_loader, criterion, optimizer, device, mixup, cutmix\n",
        "        )\n",
        "\n",
        "        # 验证\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # 更新学习率\n",
        "        scheduler.step()\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_advanced_model.pth')\n",
        "            print(f\"✓ 保存模型 (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "    # 生成预测\n",
        "    model.load_state_dict(torch.load('best_advanced_model.pth'))\n",
        "\n",
        "    # 使用高级TTA\n",
        "    predictions_df = predict_with_advanced_tta(model, test_image_paths, device, class_names, n_tta=10)\n",
        "    predictions_df.to_csv('submission_advanced.csv', index=False)\n",
        "    print(\"高级预测已保存到 submission_advanced.csv\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ==================== K折交叉验证（可选） ====================\n",
        "def train_with_kfold(n_folds=5):\n",
        "    \"\"\"\n",
        "    使用K折交叉验证训练多个模型，然后集成预测\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 数据路径\n",
        "    train_dir = '/content/FIT5215_Dataset'\n",
        "    test_dir = '/content/test_set/official_test'\n",
        "\n",
        "    # 动态获取类别名称\n",
        "    class_names = get_class_names(train_dir)\n",
        "\n",
        "    # 加载训练数据\n",
        "    train_image_paths = []\n",
        "    train_labels = []\n",
        "\n",
        "    if os.path.exists(train_dir):\n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            class_dir = os.path.join(train_dir, class_name)\n",
        "            if os.path.exists(class_dir):\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        train_image_paths.append(os.path.join(class_dir, img_name))\n",
        "                        train_labels.append(class_idx)\n",
        "\n",
        "    # 加载测试数据\n",
        "    test_image_paths = []\n",
        "    if os.path.exists(test_dir):\n",
        "        test_files = sorted(os.listdir(test_dir),\n",
        "                          key=lambda x: int(os.path.splitext(x)[0]) if os.path.splitext(x)[0].isdigit() else x)\n",
        "        test_image_paths = [os.path.join(test_dir, f) for f in test_files\n",
        "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    print(f\"训练数据: {len(train_image_paths)} 张\")\n",
        "    print(f\"测试数据: {len(test_image_paths)} 张\")\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_image_paths, train_labels)):\n",
        "        print(f\"\\n训练 Fold {fold+1}/{n_folds}\")\n",
        "\n",
        "        # 划分数据\n",
        "        train_paths = [train_image_paths[i] for i in train_idx]\n",
        "        train_labs = [train_labels[i] for i in train_idx]\n",
        "        val_paths = [train_image_paths[i] for i in val_idx]\n",
        "        val_labs = [train_labels[i] for i in val_idx]\n",
        "\n",
        "        # 创建模型并训练\n",
        "        model = PowerfulSingleModel(num_classes=len(class_names))\n",
        "        model = model.to(device)\n",
        "\n",
        "        # 这里应该添加完整的训练代码，暂时简化\n",
        "        print(f\"Fold {fold+1} 训练完成（实现略）\")\n",
        "\n",
        "        fold_models.append(model)\n",
        "\n",
        "    # 集成所有fold的预测\n",
        "    all_predictions = []\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    for img_path in test_image_paths:\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        fold_preds = []\n",
        "        for model in fold_models:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                output = model(img_tensor)\n",
        "                pred = F.softmax(output, dim=1).cpu().numpy()\n",
        "            fold_preds.append(pred)\n",
        "\n",
        "        # 投票或平均\n",
        "        final_pred = np.mean(fold_preds, axis=0)\n",
        "        all_predictions.append(np.argmax(final_pred))\n",
        "\n",
        "    # 生成预测DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'ID': range(len(all_predictions)),\n",
        "        'Label': [class_names[pred] for pred in all_predictions]\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 运行高级训练\n",
        "    model = train_advanced_model()\n",
        "\n",
        "    # 可选：K折交叉验证\n",
        "    # predictions = train_with_kfold(n_folds=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3TUkvU9bKCe_",
        "outputId": "a0e1fb2c-b899-4752-ecdd-bb5c9e4904a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "发现 20 个类别: ['birds', 'bottles', 'breads', 'butterfiles', 'cakes', 'cats', 'chickens', 'cows', 'dogs', 'ducks', 'elephants', 'fishes', 'handguns', 'horses', 'lions', 'lipsticks', 'seals', 'snakes', 'spiders', 'vases']\n",
            "训练数据: 9466 张\n",
            "测试数据: 16167 张\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 109M/109M [00:00<00:00, 212MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [1/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:42<00:00,  5.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1739, Train Acc: 32.23%\n",
            "Val Loss: 0.0371, Val Acc: 99.23%\n",
            "✓ 保存模型 (Val Acc: 99.23%)\n",
            "\n",
            "Epoch [2/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0688, Train Acc: 68.58%\n",
            "Val Loss: 0.0304, Val Acc: 99.30%\n",
            "✓ 保存模型 (Val Acc: 99.30%)\n",
            "\n",
            "Epoch [3/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0619, Train Acc: 70.18%\n",
            "Val Loss: 0.0298, Val Acc: 99.44%\n",
            "✓ 保存模型 (Val Acc: 99.44%)\n",
            "\n",
            "Epoch [4/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0595, Train Acc: 74.09%\n",
            "Val Loss: 0.0294, Val Acc: 99.37%\n",
            "\n",
            "Epoch [5/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0617, Train Acc: 70.86%\n",
            "Val Loss: 0.0293, Val Acc: 99.44%\n",
            "\n",
            "Epoch [6/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0604, Train Acc: 66.42%\n",
            "Val Loss: 0.0294, Val Acc: 99.44%\n",
            "\n",
            "Epoch [7/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0639, Train Acc: 68.47%\n",
            "Val Loss: 0.0295, Val Acc: 99.51%\n",
            "✓ 保存模型 (Val Acc: 99.51%)\n",
            "\n",
            "Epoch [8/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0602, Train Acc: 71.82%\n",
            "Val Loss: 0.0291, Val Acc: 99.58%\n",
            "✓ 保存模型 (Val Acc: 99.58%)\n",
            "\n",
            "Epoch [9/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0554, Train Acc: 67.11%\n",
            "Val Loss: 0.0293, Val Acc: 99.44%\n",
            "\n",
            "Epoch [10/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0617, Train Acc: 73.07%\n",
            "Val Loss: 0.0293, Val Acc: 99.44%\n",
            "\n",
            "Epoch [11/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0581, Train Acc: 72.64%\n",
            "Val Loss: 0.0294, Val Acc: 99.44%\n",
            "\n",
            "Epoch [12/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0576, Train Acc: 67.44%\n",
            "Val Loss: 0.0295, Val Acc: 99.01%\n",
            "\n",
            "Epoch [13/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0573, Train Acc: 72.27%\n",
            "Val Loss: 0.0295, Val Acc: 99.08%\n",
            "\n",
            "Epoch [14/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0598, Train Acc: 66.73%\n",
            "Val Loss: 0.0295, Val Acc: 99.08%\n",
            "\n",
            "Epoch [15/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0568, Train Acc: 71.53%\n",
            "Val Loss: 0.0295, Val Acc: 99.08%\n",
            "\n",
            "Epoch [16/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:34<00:00,  7.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0570, Train Acc: 69.57%\n",
            "Val Loss: 0.0294, Val Acc: 99.23%\n",
            "\n",
            "Epoch [17/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0538, Train Acc: 69.72%\n",
            "Val Loss: 0.0295, Val Acc: 99.23%\n",
            "\n",
            "Epoch [18/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0557, Train Acc: 71.39%\n",
            "Val Loss: 0.0295, Val Acc: 99.08%\n",
            "\n",
            "Epoch [19/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0556, Train Acc: 70.00%\n",
            "Val Loss: 0.0297, Val Acc: 99.23%\n",
            "\n",
            "Epoch [20/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0559, Train Acc: 69.95%\n",
            "Val Loss: 0.0294, Val Acc: 99.15%\n",
            "\n",
            "Epoch [21/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0570, Train Acc: 68.23%\n",
            "Val Loss: 0.0292, Val Acc: 99.44%\n",
            "\n",
            "Epoch [22/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0603, Train Acc: 69.66%\n",
            "Val Loss: 0.0294, Val Acc: 99.37%\n",
            "\n",
            "Epoch [23/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0549, Train Acc: 71.84%\n",
            "Val Loss: 0.0298, Val Acc: 99.23%\n",
            "\n",
            "Epoch [24/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0577, Train Acc: 67.36%\n",
            "Val Loss: 0.0293, Val Acc: 99.37%\n",
            "\n",
            "Epoch [25/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0567, Train Acc: 72.00%\n",
            "Val Loss: 0.0294, Val Acc: 99.30%\n",
            "\n",
            "Epoch [26/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0593, Train Acc: 70.38%\n",
            "Val Loss: 0.0293, Val Acc: 99.51%\n",
            "\n",
            "Epoch [27/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0573, Train Acc: 73.88%\n",
            "Val Loss: 0.0295, Val Acc: 99.51%\n",
            "\n",
            "Epoch [28/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0543, Train Acc: 70.77%\n",
            "Val Loss: 0.0292, Val Acc: 99.58%\n",
            "\n",
            "Epoch [29/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0550, Train Acc: 67.25%\n",
            "Val Loss: 0.0292, Val Acc: 99.44%\n",
            "\n",
            "Epoch [30/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:36<00:00,  6.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0567, Train Acc: 66.99%\n",
            "Val Loss: 0.0294, Val Acc: 99.51%\n",
            "\n",
            "Epoch [31/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0535, Train Acc: 71.73%\n",
            "Val Loss: 0.0293, Val Acc: 99.30%\n",
            "\n",
            "Epoch [32/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0600, Train Acc: 73.15%\n",
            "Val Loss: 0.0293, Val Acc: 99.44%\n",
            "\n",
            "Epoch [33/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0525, Train Acc: 73.02%\n",
            "Val Loss: 0.0295, Val Acc: 99.23%\n",
            "\n",
            "Epoch [34/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0563, Train Acc: 67.30%\n",
            "Val Loss: 0.0296, Val Acc: 99.23%\n",
            "\n",
            "Epoch [35/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0547, Train Acc: 72.51%\n",
            "Val Loss: 0.0301, Val Acc: 99.15%\n",
            "\n",
            "Epoch [36/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0535, Train Acc: 70.31%\n",
            "Val Loss: 0.0296, Val Acc: 99.37%\n",
            "\n",
            "Epoch [37/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0540, Train Acc: 68.74%\n",
            "Val Loss: 0.0293, Val Acc: 99.37%\n",
            "\n",
            "Epoch [38/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0548, Train Acc: 69.18%\n",
            "Val Loss: 0.0299, Val Acc: 98.87%\n",
            "\n",
            "Epoch [39/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0538, Train Acc: 69.08%\n",
            "Val Loss: 0.0296, Val Acc: 99.23%\n",
            "\n",
            "Epoch [40/40]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 252/252 [00:35<00:00,  7.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0566, Train Acc: 75.65%\n",
            "Val Loss: 0.0298, Val Acc: 99.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Advanced TTA: 100%|██████████| 16167/16167 [19:54<00:00, 13.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "高级预测已保存到 submission_advanced.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}